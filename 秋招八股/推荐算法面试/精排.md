##### 1.FTRL
- FTRL是如何保证“在线学习”的稳定性的？
	- 第t步的最优参数，不是单单最小化第t步的损失，而是让之前所有步骤的损失之和最小
- FTRL是如何保证解的稀疏性的？
	- 当z小于lambda的时候，w直接变成0
- FTRL是如何解决高维稀疏特征受训机会不均匀的问题的
	- FTRL不使用统一的步长，而是为每个特征独立设置步长。
##### 2.FM
- FM相对LR的优势在哪里
	- 自动交叉二阶特征
	- 通过隐向量替代原本二阶交叉项权重wij，解决交叉特征太稀疏、受训机会少的问题，将n^2的参数量降低到nk，减小过拟合问题
	- 增加扩展性。原本LR的二阶权重，如果训练集没有xi和xj的交叉特征，wij只能是0，但如今wij可以用vi×vj替代
- FM的复杂度是多少
	- 原本是kn^2的复杂度，但是可以优化到kn
![[1709774369156.png]]
- 缺点是什么
	- 扩展到三阶参数量容易爆炸
- 手撕
```
# 导入必要的库
import numpy as np

# 定义模型参数
num_features = 10  # 特征数量
num_factors = 5  # 隐向量维度
learning_rate = 0.01  # 学习率
num_epochs = 100  # 训练轮数
reg_lambda = 0.01  # 正则化参数

# 初始化权重和隐向量
w_0 = 0  # 偏置项
w = np.zeros(num_features)  # 线性项权重
V = np.random.normal(scale=0.1, size=(num_features, num_factors))  # 隐向量矩阵

# 定义训练数据
X = np.random.rand(100, num_features)  # 特征矩阵
y = np.random.rand(100)  # 标签向量

# 训练过程
for epoch in range(num_epochs):
    for i in range(X.shape[0]):
        xi = X[i]
        yi = y[i]

        # 计算预测值
        linear_term = np.dot(xi, w) + w_0
        interaction_term = 0.5 * np.sum(
            (np.dot(xi, V) ** 2) - np.dot(xi ** 2, V ** 2)
        )
        y_pred = linear_term + interaction_term

        # 计算误差
        error = yi - y_pred

        # 更新参数
        w_0 += learning_rate * error
        w += learning_rate * (error * xi - reg_lambda * w)
        for f in range(num_factors):
            V[:, f] += learning_rate * (
                error * (xi * np.dot(xi, V[:, f]) - V[:, f] * (xi ** 2))
                - reg_lambda * V[:, f]
            )

    # 输出当前轮的损失
    loss = np.mean((y - (np.dot(X, w) + w_0 + 0.5 * np.sum(
        (np.dot(X, V) ** 2) - np.dot(X ** 2, V ** 2), axis=1))) ** 2)
    print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}")

# 训练结束
print("训练完成")
```
##### 3.FFM
- FFM相对于FM的改进有哪些？为什么要这么改进？
	- 相同feature面对不同的feature做交叉，用不同的隐向量
	- 防止互相干扰
- FFM相比于FM的缺点在哪里
	- 参数量增多，容易过拟合，训练难度增加
##### 4.Wide & Deep
- Wide & Deep是如何做到兼顾“记忆”与“扩展”的？
	- 利用Wide部分加强模型的“记忆能力”，利用Deep部分加强模型的“泛化能力”。Wide侧发挥LR"强于记忆"的优势，把那些在训练数据中高频、大众的模式牢牢记住。此外，Wide侧的另一个作用是防止Deep侧过分扩展而影响预测精度，起到一个类似"正则化"的作用。
- 什么样的特征进Deep侧？什么样的特征进Wide侧？
	- Wide侧主要是一些历史信息或者人工设计的交叉、共现特征，以及影响推荐系统的bias特征。
	- Deep侧主要是全量的特征向量，进行深层的特征交叉，挖掘藏在特征背后的数据模式。
- 如何训练
	- Wide侧一般采用FTRL优化器，保证Wide侧解的稀疏性。
	- Deep侧采用DNN的常规优化器，比如Adagrad、Adam等。
- 和DeepFM的区别和联系
	- 联系：
		- 两者的DNN部分模型结构相同
		- 都是线性模型与深度模型的结合，低阶与高阶特征交互的融合。
	- 区别：
		- wide&deep需要做特征工程，二阶特征交叉需要靠特征工程来实现，通过wide部分发挥作用；DeepFM完全不需要做特征工程，直接输入原始特征即可，二阶特征交叉靠FM来实现
		- FM和DNN共享相同的embedding，而 Wide&Deep 的 wide 侧是稀疏输入，deep 侧是稠密输入
		- DeepFM的FM侧优化器是Adam，但是wide是FTRL
##### 5.DCN
- 提出动机：
	- 实现特征交叉，只依靠DNN这种隐式交叉是远远不够的，还应该加上显式、指定阶数的交叉作为补充。
- DCN v1和v2的差别在哪里？
	- v1的权重参数仅仅是向量，v2把d维向量w扩充为d×d维矩阵W，再将其分解为两个小矩阵相乘，降低运算和存储压力。
- DCN有哪些缺陷？
	- 原始输入的长度d一般很大，每层Cross Layer的参数量仍然不小，所以喂入Cross Network的输入一般都是经过挑选的潜在重要特征，不能把所有特征都扔进去。
	- 每层Cross Layer的输入输出都是d维，相当于只做信息交叉，而不做信息的压缩和提炼。
##### 6.Transformer做特征交叉的缺点有哪些？
- 为了使用Self-Attention，要求各Field Embedding的长度必须相等，这显然太死板了。
- 每层Transformer对信息只交叉不压缩，且复杂度随序列长度成平方增长，太慢了
##### 7.序列建模
- 你在建模行为序列中的每个元素时，一般会包含哪些信息？
	- 由每个视频的ID进行Embedding得到的向量
	- 时间差信息
	- 一些视频的特征
- 如何Embedding
	- pooling
	- 按照时间差或者完成度进行加权pooling
	- Attention
- DIN的缺点以及如何改进
	- 无法建模序列内部关系
	- ![[1709780595362.png]]
	- 无法超长度序列建模
		- 在线派
			- SIM
				- hard search
				- soft search：ANN近似搜索
			- 缺点：
				- 工程团队得牛逼
		- 离线派
			- 人工统计长期兴趣，从用户行为历史提取各类统计指标来描述用户兴趣
			- 训练一个辅助模型，输入用户长序列，输出用户的兴趣特征
				- 用同一个用户的长期行为序列，预测他的短期行为序列
				- 双塔模型
				- 喂入模型的样本是一个三元组<LSA, SSA, SSB>，LSA和SSA分别是A用户的长序列和短序列，SSB是B的短序列，训练方式按照pairwise的双塔模型训练


