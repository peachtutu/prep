##### 1.离线AUC涨了，线上没涨
- 先找自己是否犯了脑残错误
	- 代码中是否出了bug
	- 配置A/B实验时是否出错？比如和别的实验起了冲突。
	- 离线训练和评估所使用的数据集是不是太小了？
	- 离线数据集是否包含了某个特殊事件（比如节日、促销等），从而削弱了其代表性。
- 工程性差异
	- 工程bug：推荐整个链路涉及的团队至少包括前端、后端、数据、算法等，各个环节的细节取决于负责该部分的工程师的风格，整个链路还需要不同团队之间的沟通和配合，涉及个人风格和团队合作的部分，不可控性大，bug产生概率也大
	- 离线和在线特征、样本不一致问题
		- 特征不一致：当离线和在线使用两套流程获取特征时，很容易出现离线在线特征不一致的问题
			- 时间戳使用错误或不使用时间戳，离线拼接的特征不是线上推理时刻的特征，比如应该采用推理时刻的时间戳而实际采用了曝光时刻的时间戳，生成的样本中的特征将是拼接时刻的特征；
			- 在特征更新时段拼接特征，一些历史时刻的特征被覆盖，即使是根据正确的时间戳拼接特征，也无法拼接线上推理时刻的特征
			- 在线推理和离线特征处理不一致，比如在线推理对行为序列有截断处理，而离线拼接特征时不截断或者截断数量和在线不一致
			- 解决方案：
				- 可以通过训练流程的改进来解决，即将在线预估时获取到的特征落盘，离线拼接样本时不需要再通过另一套流程获取特征，只需要拼接样本的label即可
		- 特征穿越
			- 训练中使用的某些特征包含了样本发生时刻之后的信息，而这些信息往往和label有一定的相关性，导致离线训练时模型根据这类特征对label拟合得非常好，而在线推理时刻的特征只包含了到当前时刻为止的信息而没有未来的信息，导致相关性消失，模型能力明显下降
			- 特征穿越问题往往是因为特征离在线不一致问题导致，比如用户的**行为序列**特征，在线推理时使用的是到当前时刻的行为序列，而离线拼接样本时获取的是最新时刻的特征，则其行为序列的特征中则极有可能包含用户在推理时刻之后发生的正向行为，如点击、播放等，这些行为和样本label强相关，而模型在训练时利用这种相关性实现对label的准确拟合。但推理时刻模型无法获得当前时刻之后的行为，则效果下降
			- 解决方案：
				- 这类问题一般可以通过定位离在线特征一致性的问题进行改
		- 样本标签
			- 样本标签离线样本的生成，依据的是用户对曝光内容的行为，而用户产生行为到曝光之间有一定的时间差，日志系统回收用户的行为也需要一定的时间，因此需要在一个时间窗口的范围内对用户的行为日志进行处理。当时间窗口较窄时，用户发生靠后的正向行为容易被丢失，从而使正样本变成了负样本，导致离线样本和实际样本标签不一致。
			- 解决方案：
				- 缓解这类问题，可以设置一个较长的时间窗口，保证回收到用户大部分的行为，生成正确的样本，而对时间窗口之外的正向行为，可以通过正样本补发来改进正样本误拼为负样本的情况，即再生成一条正样本
		- 模型更新
			- 当模型更新较慢时，存在离在线数据分布差异较大的问题，从而导致离在线AUC不一致，当模型更新的实时性越高，离线和在线的数据分布差异越小，由分布差异导致的离在线AUC不一致问题将被缓解。
- 机制性差异
	- AUC本身问题
		- 离线AUC具有全局性，是基于训练过程中所有用户构成的全量样本计算得到的，而模型在线上推理时仅基于当前这一个用户，因此基于全局样本的离线AUC高，并不代表基于特定用户的准确性就高，则导致离线AUC涨而线上效果不佳。
		- 改进：使用颗粒度更细的GAUC
		- 虽然GAUC可以改善这个问题，但理论上，离线和在线在评估方式上仍然无法完全对齐，因为在线推理时呈现给用户的是当前这一次请求的结果，而离线评估时是用户在某个时间窗口的所有样本，若干个前者构成后者，因此在评估方式上的完全对齐，还需要考虑基于session的粒度。而基于session的粒度也会带来新的问题，比如低活用户在session内没有任何正样本，导致AUC无法计算，从而使最终评估的有效性打折，因此需要在对齐和有效性之间做一个平衡，目前采用GAUC基本可以比较好地解决该问题
	- 位置信息
		- item曝光后经过系统一系列处理得到样本，隐式地涵盖了样本所处的位置信息，一个明显的体现是ctr和位置有强关系，一般位置越靠前的item对应的点击率越高。由离线得到的AUC，评估的不是单纯的ctr，而是包含了位置信息的ctr，而模型在线上推理时，并未考虑位置信息，因此离线AUC提升表示带位置信息的ctr效果好，其中一部分效果可能是位置信息贡献的，而线上只考虑单纯的ctr时，效果不一定好，所以当模型建模时位置信息参与度越高而线上推理忽略位置因素时，该问题越容易出现
		- 解决方式：
			- 缓解该问题的方法是对位置消偏的建模，离线训练时显式地考虑位置信息，线上推理时对所有item的位置取同一个值
	- 老汤模型
		- 用大量数据追老模型
			- 慢，效率低
		- 热训练
			- 非常麻烦。新老模型之间的映射关系，没有规律可循，每次从老模型提取参数、剪裁、补齐、重填至新模型，都需要重新编写脚本与配置，费时费力还容易出错。
			- 束缚了算法工程师的手脚，使我们不愿意对模型做大幅修改，只愿意小修小补。因为改动的地方越多，热启迁移也就越繁琐。而且改动多了，很多模块的映射关系也就不复存在，压根无法迁移
			- 缺乏理论保证。很多问题，比如在尺寸不匹配时，截断或补齐，究竟要发生在头部还是尾部
	- 冰山现象
		- 我们的离线评估方法本来就是"有偏"的（biased），存在"冰山现象"
		- 就拿精排来说，无论训练还是评估，我们都拿点击做正样本，曝光未点击做负样本。所谓离线评估效果好，只不过是说新模型在曝光样本上，也就是老版推荐系统筛选出来的那部分优质物料上，表现得比老模型要好。换句话说，新模型在露出海面的那一小部分冰山尖上表现出色。但我们并不能就此就得出"新模型优于老模型"的结论，因为毕竟新模型上线后要面对的是整座冰山，而非只有山尖。
		- 对于老模型下未曝光的样本，相当于掩藏在水面下的大部分冰山，新模型从未见过，在它们身上的表现完全是个未知数。相同条件的在线预测，新模型可能会将一些本来被老模型打压、没有曝光机会的物料提升到前面，从而有机会向用户曝光。对于这部分物料，新模型既未训练过，也未测试过，对用户的反映完全靠"猜"。万一猜错了，新模型的效果打不过老模型，自然也就毫不奇怪了，"愿赌服输"嘛。
		- 本来以上缺陷也不算什么，毕竟新模型也会在线学习（Online Learning），持续更新。如果新模型猜错了，用户的负反馈会使它得到教训，从而完善自己，避免下次再犯同样的错误。但不幸的是，新模型刚上线时，都只占小流量，由新模型产生的样本是极其有限的。喂给新模型在线学习的，主要还是老模型产生的样本。在前景不明朗的时候，老板不会同意你给新模型扩大流量。所以只能在线训练新模型的时候，给新模型自己产生的样本赋予更大的权重，希望新模型聚焦学习自己犯下的错误，改正离线训练时由于"冰山现象"带来的偏差。
	- 链路一致性问题
		- 举个例子，这次我新开发了一路"封面召回"，给用户返回与他之前点击的视频拥有类似封面的视频。但是由于粗排和精排还没有用上封面信息，所以在它们眼里，"封面召回"的视频与用户兴趣根本不搭边，从而将它们筛掉。"封面召回"最终有机会呈现给用户的结果寥寥无几，自然不可能在A/B实验指标上体现出什么效果。
		- 解决方法就是让模型不仅要拟合用户的兴趣爱好，还要迎合下游模型的口味。
- 扩大流量看看，有可能是小流量波动问题
##### 2.离线评估和在线评估的区别
- 离线评估的优点是简单快捷，不需要回溯较长时间的历史数据，也不用等待用户反馈。缺点是离线指标只是评估模型的优劣，并不能直观反映对业务的影响。另外，离线评估是在一个简化（训练集与测试集都比较小）、理想（新老模型有相同的初始条件）的环境下进行的，不能反映线上的真实环境，所以离线指标的提升未必能带来线上业务指标的提升。
- 在线评估的优点是，线上真实的环境，真实的用户反馈，能比较客观反映模型对业务的影响。缺点是，准备阶段回溯历史要等，上线后搜集用户反馈也要等，时间成本非常高。
##### 3.如何评估特征重要性
- Ablation Test：在N个特征每次除去一个特征，看效果。
	- 缺点是太浪费时间
- Permutation Test：训练好模型后，在测试集上每个特征都打乱一下，看和正常测试机的区别，区别越大，说明这个特征越重要
	- 缺点：有些特征之间并非独立，而是相互关联，只打散其中一列，可能产生完全不符合常量的样本。比如推荐模型所使用的"物料分类"与"物料标签"就是强关联的，打散"标签"特征，可能会制造出分类是音乐、标签却是美食的问题物料。在这种特征分布与现实严重不符的样本上测试，得到的评估指标也就失去了意义，由此衍生出的特征重要性也要打上大大的问号。
- Top-Bottom Analysis：将测试集预测的结果，按照打分从高到低排列，选取前N和后N的数据，看特征差别，差距越大，说明越重要
- 解释模型：SENet
##### 4.如何评估模块重要性
- 可视化方法
	- 将每层输出的N个向量，利用t-SNE算法降维至2维空间，再画成散点图观察
- 模型
	- 用每层输出的N个向量拟合一个Logistic Regression模型，然后比较各层拟合出来的LR模型的性能。注意，这里特意拟合LR而非更复杂的模型，就是为了让拟合模型的性能差异体现在输入向量的质量上，而不是拟合模型的功劳




