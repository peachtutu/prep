##### 1.协同过滤
- Item CF更常用
- item CF VS user CF
	- 从算法的复杂度角度来讲，如果用户的个数大于物品的个数，采用基于物品的协同过滤要优于基于用户的协同过滤。
	- 相似度计算的**准确度**的角度来讨论这个问题。同样以电影评分网站为例，单个用户的数据往往较少，平均每个用户评分过的电影数量不超过几十，而单个电影的评分数据往往超过1千。因此，考虑到数据的稀疏性会影响到相似度计算的可信度，在计算电影物品相似度的时候，有效评分数据的量级要大于计算用户相似度的数据量级。所以从相似度计算准确角度而言，基于物品的协同过滤优于基于用户的协同过滤。综上可以得出结论，当物品的数量远小于用户的数量时，不论是从计算复杂度，还是从相似度计算可信度的角度，基于物品的协同过滤都要比基于用户的协同过滤更好。
	- 当用户的数量和物品的数量相差不大，并且平均每个用户积累的评分行为数据较多，特别是用户的兴趣比较稳定的情况下，基于用户的协同过滤也能表现出很好的效果。
##### 2.双塔模型三种训练方式
- pointwise
	- 训练样本是二元组（user，item+）or（user，item-）
	- 把召回看成二元分类
		- 对于正样本，鼓励cos（a，b+）接近+1
		- 对于负样本，鼓励cos（a，b-）接近-1
		- 控制正负样本比例为1：2或1：3
- pairwise
	- 训练样本是三元组（user，item+，item-）
	- 鼓励cos（a，b+）大于cos（a，b-）
	- 损失函数：
		- hinge loss：针对同一个用户u，正例物料与他的匹配程度，要远远高于，负例物料与他的匹配程度
		- BPR loss：BPR的思想是给定一个由由用户、正例物料、随机采样的负例物料组成三元组，针对用户的正确排序的概率是sigmoid（ui+ - ui-），BPR Loss就是要将这一正确排序的概率最大化
![[1709699633006.png]]
![[1709699653844.png]]
- listwise
	-  训练样本是n元组（user，item+，item-，item-，item-，...）
	-  鼓励cos（a，b+）尽量大
	-  鼓励cos（a，b-）尽量小
		- softmax
			- 温度修正
				- 可以起到一个放大器的使用，但凡有哪个负例没被训练好，就能把这个"错误"放大许多倍，导致分母变大，损失增加。那个没被训练好的负例，就会被模型聚焦，被重点"关照"
				- 如果设置得很小，它对错误的放大功能就很强。推荐结果的精度高，但对用户潜在兴趣覆盖得不够，容易使用户陷入"信息茧房"而疲劳。
				- 如果设置大了，它对错误的放大功能就很弱。好处是，真的覆盖了用户的一部分隐藏兴趣，推荐是对用户兴趣的一次探索与扩展，丰富推荐结果的多样性，打破"信息茧房"。坏处是，召回把关太松，有损用户体验。
		- NCE
			- Noise Contrastive Estimation（NCE）是简化分母的一种思路，它将召回原始的"超大规模多分类问题"简化为一系列"区别样本是否来自噪声"的二分类问题
		- NEG
			- 为了进一步简化计算，干脆就在公式中忽略掉修正项，得到了NEG Loss
			- NEG Loss的优点是计算简便，而它的缺点是不像NCE那样有非常强的理论保证。已经证明，如果采样的负例足够多，NCE的梯度与原始超大规模Softmax的梯度趋于一致，但是NEG没有这种性质。但是由于我们的目标并不是为了将概率分布学准确，而是为了学习出高质量的用户向量与物料向量，因此NEG在理论上的瑕疵并不是大问题，照样在召回场景中被广泛应用。
![[1709536811690.png]]
![[1709701757693.png]]
![[1709701780355.png]]
##### 3.负样本选取
- 简单负样本
	- 全局采样，与热门程度正相关，越热越容易成为负样本
		- 比如抽样概率正比于（点击次数）^0.75
	- batch内负样本
		- 这里抽样概率是正比于（点击次数）^1，太狠了，为了缓解，选择如下
![[1709535620117.png]]
- 困难负样本
	- 粗排和精排淘汰的
	- 曝光未点击
##### 4.[[双塔的缺点]]
##### 5.Airbnb
- I2I
	- 正样本：除了在一个session内的两两算正样本（窗口内，否则太多），如果一个点击序列最终导致某个房屋成功预订，业务信号非常强，必须保留
	- 负样本：但是"民宿中介"的业务特色决定了，在一个点击序列中的房屋（i.e.，正样本）基本上都是**同城**的，而随机采样得到的负样本多是**异地**的。如果只有随机负采样，模型可能只使用"所处城市是否相同"这一粗粒度差异来判断两房屋相似与否，导致最终学到的房屋向量按所在城市聚类，而忽视了同一城市内部不同房屋的差异。为了弥补随机负采样的不足，Airbnb还为每个房屋在与其**同城的其他房屋**中采样一部分作为Hard Negative，迫使模型关注"所在城市"之外的更多细节。
- U2I
	- 如何解决样本稀疏问题
		- 根据属性和人工规则，将用户和房屋**分类**，单个用户与单个房屋的预订记录是稀疏的，但是某类用户与某类房屋的预订记录就丰富许多
##### 6.EGES
- word2vec在推荐领域内的正样本有局限性
	- 都认为只有被同一个用户在同一个Session内交互过的两个物料之间，才有相似性，才可能成为正样本，无法跨session。比如A和B在一个session，B和C在一个session，那A和C可能相似
- 只用了ID特征，没用其他的特征
- EGES如何解决
	- 针对正样本
		- 先根据用户行为序列，建立物料关系图
		- 图上的边随机游走，产生一批新的序列，概率正比于边的权重，边eij的权重是先点击i再点击j的次数
		- 在这些随机游走产生的新序列上，再套用Word2Vec的方法，定义滑窗，窗口内的两个物料是相似的，成为正样本
	- 针对只用ID特征
		- 生成1+n个embedding矩阵，n是特征数量，而这些特征，不像ID embedding对于新item拿不到。因此训练时，n+1个embedding做pooling或者加权平均，预测时如果没有id embedding，可以只用其他n个embedding
##### 7.随机负采样怎么做
- 离线采样
- batch内负采样
	- 优点：由于ui的某个负例tj的向量，已经作为ut的正例，被计算过了，可以被复用而避免重复计算，计算量降低
	- 缺点：召回的正样本来自点击数据，而被点击的多是热门物料。采集到的负样本都是Hard Negative，缺少Easy Negative
- 混合负采样
	- 额外建立了一个"向量缓存"，存储"物料塔"在训练过程中得到的最新的物料向量。
	- 在训练每个Batch的时候，先进行Batch内负采样，同一个Batch内两条样本中的物料互为Hard Negative。
	- 额外从"向量缓存"采样一些由"物料塔"计算好的之前的物料向量，作为Easy Negative的Embedding。
##### 8.GraphSAGE
- 在GraphSAGE出现之前，GCN的实现都是"直推式"（Transductive）的，也就是预测与训练只能使用相同的图，对于未曾在训练图上出现过的节点，传统GCN无法给出它们的向量表示。
- GraphSAGE并不直接学习图上各节点的向量，而是要学习出一个转换函数。只要输入节点的特征和它的连接关系，该转换函数就能返回该节点的向量表示。所以，GraphSAGE是"归纳式"（Inductive）的，能够在训练时未曾出现过的新节点上，也获得向量表示。
![[1709704393806.png]]
##### 9.如何在一个超大规模图上，训练GCN召回模型？
- Mini-Batch训练
	- 每轮训练时，GPU只加载计算当前Mini-Batch所需要的子图
- 邻居采样
	- 生成各层子图时，并没有采用u的全部邻居，而是从u的邻居中抽样得到N(u)
	- PinSage采用的是基于"随机游走"（Random Walk）的采样方式，也就是由图上某个节点p开始，进行若干次随机游走，并记录游走过程中访问到的其他节点和访问次数。最后，选择访问次数最多的前T个邻居作为采样结果
- 推理和训练不一样
	- 训练时生成Embedding的过程，并不适用于推荐预测
	- 图上有上亿个节点，单机计算肯定来不及，必须采用分布式并发计算。
	- 原因一，代码中采用了"邻居采样"而引入了随机性，预测时必须聚合节点的全部邻居。
	- 原因二，代码是基于Mini-Batch的。尽管两个Batch所包含的节点不同，但是为了计算这两个Batch而生成的各层子图，很可能包含了相同的邻居节点。这些共享邻居节点上的卷积结果被反复计算，浪费了算力