##### 1.随机森林的随机性体现在哪里？
- 体现在样本随机性和特征随机性两个方面：
	- 样本随机性指的是随机而且有放回地从训练集选取N个（原训练集大小为N）训练样本作为训练集，所以每棵树训练集都不一样，但包含重复样本；
	- 特征随机性指的是每次训练决策树的时候，会从所有特征中选取部分特征进行训练，降低每棵树的相关性。
##### 2.优缺点
- 优点
	- 高度准确性：随机森林能够产生较高的预测准确性，因为它由多个决策树组成，通过集体投票或平均预测来确定最终结果，从而减少了单个决策树的过拟合风险。
	- 抗噪能力强：随机森林对于数据中的噪声和异常值具有较好的鲁棒性，它可以通过多棵决策树的集成来减少单个决策树的错误。
	- 处理高维数据：随机森林可以处理具有大量特征的高维数据，它能够自动选择重要特征，减少冗余特征的影响，并提供有关特征重要性的信息。
	- 可解释性：随机森林可以提供特征重要性评估，它能够告诉我们哪些特征对于模型的预测起到了重要作用。
- 缺点
	- 训练时间长：随机森林由多个决策树组成，训练时间较长，尤其在处理大规模数据集时。
	- 占用内存较大：由于随机森林包含多个决策树，需要占用较大的内存空间存储模型。
	- 预测时间长：在进行预测时，需要遍历所有决策树并进行投票或平均，因此预测时间较长。
##### 3.随机森林为什么不能用全样本取训练m棵决策树？
- 随机森林算法的核心思想在于通过引入随机性来构建多个决策树，以此来提高模型的泛化能力并减少过拟合的风险，如果使用全样本来训练每一棵决策树，就做不到这样的效果

bagging是降低方差，boosting是降低偏差