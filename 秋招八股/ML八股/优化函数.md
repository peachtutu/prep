##### 1.不同优化器比较
- DG
	- BGD  使用整个训练集
		- 训练速度慢，内存占用高，稳定性好
	- SGD  使用一个样本
		- 训练速度快，内存占用低，稳定性差
	- MiniBGD  使用部分样本
		- 介于BGD和SGD之间
- Momentum 引入一阶动量
		$$\begin{aligned}
		m_{t}=\beta m_{t-1}+(1-\beta) \cdot g_{t}\\
		w_{t+1}=w_{t}-\eta \cdot m_{t}
		\end{aligned}$$
		引入一阶梯度，利用历史梯度信息，加速收敛，减小震荡
- Adagrad
		$$\begin{aligned}
		v_{t}=\sqrt{\sum_{\tau=1}^{t}g_{\tau}^{2}+\epsilon}\\
		\eta_{t}=\frac{\eta_{g l o b a l}}{v_{t}}\\
		w_{t+1}=w_{t}-\eta_{t} \cdot g_{t}
		\end{aligned}$$
		引入二阶矩，对学习率进行动态调整，但是需要人为设置global的参数，并且到了后期，vt过大导致学习率变为0，停止训练
- RMSProp
			$$\begin{aligned}\\
		v_{t}= \rho v_{t-1} +  \left ( 1-\rho  \right ) \cdot g_{t}^{2} \\
		\eta _{t} = \frac{\eta _{global} }{\sqrt{v_{t} +  \epsilon } } \\
		w_{t+1}=w_{t}-\eta_{t} \cdot g_{t}
		\end{aligned}$$
		为了改进Adagrad的缺点，vt不再累计全局梯度，而是只保留过去一部分梯度，缓解了vt过大导致学习率变0的问题，但是仍然需要人为设置global的参数
- Adam
		$$\begin{aligned}\\
		m_{t} = \beta _{1} \cdot m_{t-1} +  \left ( 1-\beta _{1}  \right ) \cdot g_{t}  \\
		v_{t} = \beta _{2} \cdot v_{t-1} +  \left ( 1-\beta _{2}  \right ) \cdot g_{t}^{2}   \\
		m_{t}^{\ast } = \frac{m_{t} }{1- \beta _{1}^{t} } \\
		v_{t}^{\ast } = \frac{v_{t} }{1- \beta _{2}^{t} } \\
		w_{t+  1} =w_{t} - \eta \frac{m_{t}^{\ast } }{\sqrt{v_{t}^{*} } +\epsilon } 
		\end{aligned}$$
		引入二阶矩和二阶矩，动态调整学习率
- SGD 和 Adam 谁收敛的比较快？谁能达到全局最优解？
	- SGD 算法没有动量的概念，SGD 和 Adam 相比，缺点是下降速度慢，对学习率要求严格。
	- Adam 引入了一阶动量和二阶动量，下降速度比 SGD 快，Adam 可以自适应学习率，所以初始学习率可以很大。
	- SGD 相比 Adam，更容易达到全局最优解。主要是后期 Adam 的学习率太低，影响了有效的收敛。
	- 我们可以前期使用 Adam，后期使用 SGD 进一步调优。
##### 2.batch size的影响
- 大batch size
	- 优点
		- 内存利用率提高，模型训练速度快
		- 减小震荡，训练过程稳定
	- 缺点
		- 内存放不下
		- 跑完一个epoch迭代次数减少，所以要达到相同精度，需要更多epoch
		- 容易陷入局部最优解
- 小batch size
	- 优点
		- 内存需求低
		- 具有较大的随机性，可以摆脱局部最优点
	- 缺点
		- 训练速度慢
		- 容易震荡
##### 3.网络收敛很慢
- 调整学习率
- 正则化和批量归一化
	- 批量归一化（Batch Normalization）：通过规范化每层的输入，有助于加速收敛，同时减少对初始权重的依赖。
	- 正则化技术：如L1、L2正则化或Dropout，可以防止过拟合，提高模型的泛化能力，间接影响收敛速度。
- 使用如He初始化或Xavier初始化的方法
- 更换激活函数
- 使用预训练模型
- 数据增强
- 减少模型复杂度
- 使用更大的批量大小，增加批量大小可以减少权重更新的方差，提高训练的稳定性。但需注意，过大的批量可能会导致内存问题，且收益递减。
##### 3.网络为什么要初始化
- 防止梯度消失或梯度爆炸：权重初始化的方式会影响到反向传播过程中梯度的传播。如果初始化不当，梯度可能会迅速衰减（消失）或者迅速增大（爆炸），导致训练过程中的数值不稳定。
- 加速收敛：良好的初始化方法可以使模型更快地收敛到最优解，从而加速训练过程。
- 避免对称性破坏：如果所有权重初始化为相同的值，神经元将对输入数据做出相同的响应，导致网络无法学习复杂的特征。通过随机初始化权重，可以破坏这种对称性，使每个神经元学习不同的特征。