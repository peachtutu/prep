##### 1.优缺点
- 优点：
	- 易于理解和解释：决策树的构建过程类似于人类的决策过程，易于理解和解释，可以提供可解释性的规则。
	- 可处理离散和连续特征：决策树可以处理各种类型的特征，包括离散特征和连续特征。
	- 能够处理多输出问题：决策树可以处理多输出问题，而不仅仅是二分类或多分类问题。
	- 对异常值和缺失值具有鲁棒性：决策树对于异常值和缺失值具有一定的鲁棒性，能够处理这些数据异常情况。
- 缺点
	- 容易过拟合：决策树容易过拟合训练数据，特别是在处理复杂问题时，可能会生成过于复杂的树结构，导致过拟合。
	- 不稳定性：对于数据的小变化非常敏感，稍微改变训练数据可能会导致完全不同的树结构，这使得决策树在一些情况下不够稳定。
	- 高计算复杂度：决策树的构建过程需要遍历特征空间，计算信息增益或基尼系数，对于大规模数据集而言，计算复杂度较高。
	- 局部最优问题：决策树的构建是一个贪心算法，可能不会达到全局最优。
##### 2.分裂方式
- ID3
	- 使用信息增益
		- 缺点：
			- 信息增益会往选择值较多的属性偏好
			- 不能处理连续性特征，需要离散化处理
			- 无法处理缺失值
			- 没有提供剪枝策略，容易过拟合
- C4.5
	- 使用信息增益比
		- 相比于ID3的改进：
			- 解决了ID3会往选择值较多的属性偏好
			- 可以直接处理连续特征，找最优划分点
			- 可以处理缺失值，
			- 提供后剪枝的策略
- CART
	- 使用基尼系数
		- 既可以用于回归也可以用于分类
		- 是二叉树
##### 3.剪枝方式
- 预剪枝
	- 最大深度限制：设定决策树的最大深度，超过该深度则停止划分。
	- 叶子节点数量限制：设定每个父节点的最大叶子节点数量，超过限制则停止划分。
	- 信息增益阈值：设定信息增益的最小阈值，小于阈值则停止划分。
	- 由于是贪心策略，速度快，不一定能保证全局最优
- 后剪枝
	- 成本复杂性剪枝（Cost Complexity Pruning）：也称为弱化剪枝，涉及使用参数α（复杂性参数）来权衡树的深度和拟合程度。它试图找到最佳的α值，以最小化决策树整体的成本复杂性。
	- 误差降低剪枝（Error Reduction Pruning）：移除分支时，如果该操作会导致训练集误差的减少，则执行剪枝。
	- 最小误差剪枝（Minimum Error Pruning）：在每个节点处评估剪枝前后的误差，如果剪枝不会导致误差显著增加，则进行剪枝。
	- 悲观剪枝（Pessimistic Error Pruning）：基于统计测试对剪枝后的误差增加持悲观态度，如果剪枝能在统计意义上减少误差，则进行剪枝。
	- 速度慢





