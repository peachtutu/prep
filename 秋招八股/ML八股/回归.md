##### 1.LR
- 推导
	- [13. 机器学习——回归 - 牛客网 (nowcoder.com)](https://www.nowcoder.com/issue/tutorial?zhuanlanId=qMKkxM&uuid=10301781a8e74f93b386addd5d10639f)
- 是否需要归一化
	- 需要，尽管逻辑回归本身不受量纲影响，但是其使用梯度下降法求解参数受量纲影响大，如果不进行特征归一化，可能由于变量不同量纲导致参数迭代求解缓慢，影响算法速率
- 为什么要做特征离散化
	- 进行非线性关系建模：逻辑回归本身是一种线性模型，通过离散化特征，可以引入非线性关系，增强模型对非线性特征的建模能力
	- 减少过拟合：连续型特征的取值范围广泛，离散化可以减少特征空间的维度，从而减少模型的复杂度，避免过拟合的问题
	- 处理异常值：离散化可以将连续型特征的异常值归为某个特定的区间，减少异常值对模型的影响。
- 为什么不用MSE损失函数
	- 逻辑回归的目标是对样本进行二分类，输出的预测值表示样本属于某一类别的概率。对数损失函数和交叉熵损失函数能够更好地衡量概率预测的准确性，相比之下，平方损失函数偏向于对预测值和真实值之间的距离进行建模，不太适合处理概率预测问题
	- 平方损失函数对离群点敏感，因此在逻辑回归中使用平方损失函数容易受到异常值的影响
	- 逻辑回归中使用的交叉熵损失函数具有凸性质，使得优化算法可以更有效地找到全局最优解，同时交叉熵的梯度和sigmoid无关，具有当误差大的时候，权重更新快；当误差小的时候，权重更新慢的性质
	- LR就是是用MLE推导的，MLE最大等价于交叉熵损失最小
- 可以处理非线性情况吗
	- 可以，一种常见的方法是引入多项式特征，例如，可以将原始的特征进行平方、交叉乘积等操作，增加更多的非线性组合
	- 还可以使用核技巧（Kernel Trick）将逻辑回归扩展到非线性情况。通过使用核函数将输入特征映射到高维空间中，可以使线性模型在原始特征空间中表现出非线性的决策边界
- 如何实现多分类
	- 将sigmoid变成softmax
	- 将多酚类任务分解为多个二分类任务
		- OvR：每次将一类作为正类，其他都作为负类，这样训练N个二分类器即可，预测时产生N个结果，选择最可能的
- 逻辑回归模型训练过程中，需要去掉高度相关的特征吗
	- 需要
		- 去掉高度相关的特征会让模型的可解释性更好
		- 以大大提高训练的速度，因为维度减少了
- 为什么要用sigmoid激活函数
	- 这是因为逻辑回归主要用于二分类问题，而Sigmoid函数能够将输入的连续实值映射到(0,1)区间内，使其可以被解释为概率。这个特性使得Sigmoid函数非常适合于逻辑回归模型，用于预测一个事件发生的概率。
- 手撕逻辑回归
```
import numpy as np

class LR:
	def __init__(self, lr=0.01, iter=1000):
		self.lr = lr
		self.iter = iter
		self.weights = None
		self.bias = None

	def sigmoid(self, z):
		return 1 / (1 + np.exp(-z))

	def fit(self, X, y):
		num_samples, num_features = X.shape
		self.weights = np.zeros(num_features)
		self.bias = 0
		for _ in range(self.iter):
			y_pred = self.sigmoid(np.dot(X, self.weights) + self.bias)
			d_w = (1 / num_samples) * np.dot(X.T, y_pred - y)
			d_b = (1 / num_samples) * np.sum(y_pred - y)
			self.weights -= self.lr * d_w
			self.bias -= self.lr * d_b

	def predict(self, X):
		y_pred = np.dot(X, self.weights) + self.bias
		y_pred = self.sigmoid(y_pred)
		return np.round(y_pred)
```
- 手撕线性回归
```
import numpy as np

class LR:
	def __init__(self, lr=0.01, iter=1000):
		self.lr = lr
		self.iter = iter
		self.weights = None
		self.bias = None

	def fit(self, X, y):
		num_samples, num_features = X.shape
		self.weights = np.zeros(num_features)
		self.bias = 0
		for _ in range(self.iter):
			y_pred = np.dot(X, self.weights) + self.bias
			d_w = (1 / num_samples) * np.dot(X.T, y_pred - y)
			d_b = (1 / num_samples) * np.sum(y_pred - y)
			self.weights -= self.lr * d_w
			self.bias -= self.lr * d_b

	def predict(self, X):
		y_pred = np.dot(X, self.weights) + self.bias
		return y_pred
```

