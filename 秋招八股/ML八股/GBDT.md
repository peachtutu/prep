##### 1.GBDT
- 模型原理：
	- 初始化模型：首先，将整个训练集作为初始模型进行训练，通常使用一个简单的模型作为初始模型，例如单个决策树。
	- 迭代训练：在每一轮迭代中，根据上一轮的模型预测结果与真实标签之间的差异（残差），训练一个新的决策树来拟合残差，一般使用负梯度来拟合。
	- 更新模型：将新的决策树添加到模型中，并更新整体模型的预测结果。
	- 终止条件：重复迭代训练过程，直到达到预定的迭代次数或满足终止条件。
- 如何做正则化
	- 定义每一轮的学习步长v
	- 定义样本子采样比例r
	- 控制树的数量或者深度
- 优缺点：
	- 优点
		- 高预测准确率：GBDT 在处理各种任务时表现出色，包括分类和回归。它能够通过多轮迭代不断优化模型，提高预测准确率。
		- 处理复杂特征交互：GBDT 可以自动捕捉特征之间的复杂交互关系，无需人为进行特征工程。
		- 鲁棒性：GBDT 对于噪声和异常值相对较为鲁棒，它通过多棵决策树的组合减少了单棵树的过拟合风险。
		- 可解释性：GBDT 能够提供特征的重要性排序，帮助理解模型对于预测的贡献程度。
	- 缺点：
		- 训练时间较长，难以并行化：GBDT 的训练过程是顺序迭代的，每轮迭代需要构建一个新的决策树。因此，相对于一些快速训练的模型，如线性模型或浅层神经网络，GBDT 的训练时间较长。
		- 参数调节难度较高：GBDT 中有多个参数需要调节，如树的数量、树的深度、学习率等。不恰当的参数选择可能导致模型性能下降。
		- 需要大量内存：由于 GBDT 是一种集成模型，它需要存储多棵决策树的结构和参数信息，因此占用较大的内存空间。
- GBDT 的 G 梯度的向量长度为多少
	- G 梯度的向量长度与训练样本的数量相同。具体而言，如果有 N 个训练样本，则 G 梯度的向量长度为 N。
- GBDT有正则化手段吗
	- 定义每一轮的学习步长v
	- 定义样本子采样比例r
	- 对每一轮的决策树进行剪枝，或者限制树的深度
- 为什么拟合的是负梯度而不是残差
	- 灵活性：适用于任何可微损失函数，不局限于MSE。
	- 准确性：通过拟合负梯度，可以更准确地反映损失函数的变化，从而更有效地最小化损失。
	- 通用性：能够处理回归、分类、排序等多种任务，只需要相应地定义损失函数。
- 如何衡量特征的重要性
	- **基于分裂指标或者分裂次数**：
	    - 每棵决策树在构建过程中，都会通过某种标准（如基尼不纯度、信息增益等）选择最优的分裂特征。
	    - 每次分裂时，选择的特征会导致不纯度的减少或信息增益的增加。
	    - 随机森林通过统计所有决策树中各个特征导致的不纯度减少（或信息增益）的累积值，来计算特征的重要性。
	- **基于置换重要性（Permutation Importance）**：
	    - 先训练随机森林模型，并计算其在验证集上的基准准确率。
	    - 然后，随机打乱一个特征的值，保持其他特征不变，并再次计算模型在验证集上的准确率。
	    - 特征的重要性通过模型准确率的下降程度来衡量。准确率下降越多，表明该特征越重要。
##### 2.XGBoost
- 优缺点
	- 优点
		- 精度更高：XGBoost 对损失函数进行了二阶泰勒展开，增加了精度
		- 正则化+列抽样：有助于防止过拟合
		- 缺失值处理
		- 支持并行计算
	- 缺点
		- 计算资源要求高： XGBoost 在大规模数据集上的训练需要较大的计算和内存资源，可能对计算环境有一定要求。
		- 参数调优复杂： XGBoost 有多个参数需要进行调优，包括树的深度、学习率、正则化参数等，需要进行交叉验证等技巧来选择最佳参数组合。
- 正则化
	- L1 正则化项：叶节点个数
	- L2 正则化项：叶节点权重平方和
- 如何处理缺失值
	- 理论上最优肯定是考虑所有情况，但是计算过于复杂，因此XGBoost采取把所有缺失值归于左边和归于右边两种，哪种gain大，放哪里
	- 预测时，按照训练时的方向选择
- 为什么要二阶展开
	- 使用二阶导数信息可以帮助XGBoost算法更快地收敛
	- 准确
- XGBoost 相对于 GBDT 的改进点
	- 正则化项： XGBoost引入了 L1正则化（Lasso）和L2正则化（Ridge）作为模型的正则化项，用于控制模型的复杂度，防止过拟合。
	- 损失函数的二阶导数近似： XGBoost通过对损失函数的二阶导数进行近似（二阶泰勒展开），提高模型的训练速度和效率。这种近似方式能够更准确地估计每个叶子节点的分裂增益。
	- 并行计算：虽然树模型本身难以并行化，但XGBoost对特征的分割点寻找过程进行了优化，通过预排序和近似算法（如直方图算法）在特征维度上进行并行计算，从而大大提高了算法的执行速度
	- 缺失值处理： XGBoost 能够自动处理缺失值，无需对缺失值进行填充或删除。XGBoost 使用一种特殊的方式来处理缺失值，在树的分裂过程中将缺失值分配给左子树或右子树。
	- 列采样：传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机森林相似的策略，支持对数据进行采样，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。
- XGBoost如何选择最佳分裂点
- ![[1708849816977.png]]
	- 暴力
		- 枚举所有可能，不现实
	- 贪心
		- 二重循环，遍历每个特征，每个特征从小到大排序，按照每个特征值进行划分左右
	- 近似
		- 压缩特征数
			- 桉树随机
			- 按层随机
		- 压缩切分点个数
			- 按照二阶导数值作为权重进行分桶
			- 同样跟列采样一样，分为全局策略和局部策略
- XGBoostt防止过拟合的方法
	- 正则
	- 列抽样和行抽样
	- shrink
- XGBoost并行方式
	- Block结构: XGBoost 中的 block 结构是指模型内部的一种数据组织形式，它有助于提高训练速度和内存利用效率。在树生成过程中，最耗时的一个步骤就是在每次寻找最佳分裂点时都需要对特征的值进行排序。而 XGBoost 在训练之前会根据特征对数据进行排序，然后保存到块结构中
	- 特征并行: 在特征并行中，XGBoost在构建决策树的每一层时，并行地考虑所有特征来寻找最佳分割点。这意味着不同的特征可以在不同的处理器上同时计算其最优分割点，然后在所有计算完成后选出全局最优的分割点。这种方法可以显著减少单棵树构建的时间。
	- 数据并行, 数据并行主要通过在多个处理器上分布式处理数据集来实现。XGBoost在每轮迭代时，将数据集分成多个子集，分配给不同的处理器。每个处理器计算其分配的数据子集上的梯度统计信息，然后这些局部统计信息被汇总以计算全局最优的分割点。这种方式可以有效地处理大规模数据集。
	- 切分点查找的近似算法, XGBoost实现了一种基于分位数的近似算法来查找最佳切分点，这个过程可以并行执行。通过将连续特征的值域划分为多个桶或分位数，XGBoost可以在这些离散化的桶上并行计算梯度信息，从而快速找到近似的最优分割点
- 每个点的值是多少
![[1719324512608.png]]
##### 3.LightGBM
- 优缺点：
	- 优点：
		- 高效和快速：LightGBM使用基于直方图的决策树分裂算法，这种方法比传统的决策树算法更快，因为它减少了计算量，并可以并行处理。
		- 低内存使用：通过合并相近的特征值来构建直方图，LightGBM能够显著减少内存的使用。
		- 处理大规模数据：得益于其高效的算法和低内存使用，LightGBM能够处理大规模数据集。
		- 支持类别特征：LightGBM能够直接处理类别特征，无需预先进行独热编码，这减少了数据的维度并提高了效率。
		- 自动处理缺失值：LightGBM可以自动处理缺失值，无需事先填充。
	- 缺点：
		- 过拟合风险：尽管LightGBM提供了用于控制过拟合的参数，但其快速学习和复杂模型的能力也可能导致过拟合，特别是在数据量较小的情况下
		- 参数调优：虽然提供了丰富的参数，但参数调优可能会比较复杂，需要较多的机器学习知识和经验来获得最佳的模型性能。
- LightGBM相对XGBoost的改进
	- XGBoost采用的是level-wise的分裂策略，而lightGBM采用了leaf-wise的策略（需要限制深度）
	- LightGBM使用了基于histogram的决策树算法
	- 可以自动处理类别特征
	- 直方图做差加速，一个子节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算。
	- 单边梯度采样算法（GOSS）
		- GOSS算法从减少样本的角度出发，排除大部分小梯度的样本，仅用剩下的样本计算信息增益，它是一种在减少数据量和保证精度上平衡的算法。
		- GOSS首先将要进行分裂的特征的所有梯度取值按照绝对值大小降序排序，选取绝对值最大的a个数据。然后在剩下的较小梯度数据中随机选择b个数据。接着将这b个数据乘以（1-a）/b，这样算法就会更关注训练不足的样本，而不会过多改变原数据集的分布。最后使用这（a+b）个数据来计算信息增益。
	- 互斥特征捆绑算法（EFB）
		- 为了减少特征维度
		- 可以捆绑完全互斥的特征，或者冲突率在限制范围内的不完全互斥的特征
##### 手撕GOSS
```
def GOSS(data, gradients, a, b):
    # 样本数量
    n = len(data)
    
    # 计算梯度的绝对值
    abs_gradients = [abs(g) for g in gradients]
    
    # 按梯度的绝对值进行排序
    sorted_indices = sorted(range(n), key=lambda i: abs_gradients[i], reverse=True)
    
    # 保留大梯度样本
    top_k = int(a * n)
    large_gradient_indices = sorted_indices[:top_k]
    
    # 对小梯度样本进行随机采样
    remaining_indices = sorted_indices[top_k:]
    small_gradient_sample_size = int(b * n)
    small_gradient_indices = random.sample(remaining_indices, small_gradient_sample_size)
    
    # 合并大梯度样本和小梯度样本
    sampled_indices = large_gradient_indices + small_gradient_indices
    
    # 根据采样的索引获取采样后的数据集
    sampled_data = [data[i] for i in sampled_indices]
    
    # 对小梯度样本进行权重调整（使其对模型的影响等效于其在原始数据中的影响）
    scale_factor = (1 - a) / b
    for i in small_gradient_indices:
        sampled_data[i].weight *= scale_factor
    
    return sampled_data
```