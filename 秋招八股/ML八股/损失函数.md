###### 1.在用 sigmoid 作为激活函数的时候，为什么要用交叉熵损失函数，而不用均方误差损失函数？
- 梯度消失问题，sigmoid的梯度在0-0.25之间，如果用MSE，那么反向传播过程中，包含了sigmoid的梯度，容易导致梯度消失问题，相反如果用交叉熵，梯度中不包含sigmoid的梯度，没有这个担忧
- 凸优化，交叉熵配合sigmoid是凸优化问题，有全局最优解，而MSE不是，容易产生局部最优解
- 更快收敛，交叉熵配合sigmoid，梯度和误差成正比，误差大时候更新快，误差小的更新慢
##### 2.sigmid和softmax函数求导的推导
![[1709164013579.png]]
##### 3.softmax可能的问题
- 数值溢出：
	- 计算前，减去最大值
##### 4.手撕softmax
```
def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1, keepdim=True)
    return X_exp / partition
```
##### 5.手撕交叉熵
```
import numpy as np

def categorical_cross_entropy_loss(y_true, y_pred):
    """
    计算多分类问题的交叉熵损失

    参数：
    y_true (np.ndarray): 真实标签，形状为 (n_samples, n_classes)
    y_pred (np.ndarray): 预测概率，形状为 (n_samples, n_classes)

    返回：
    float: 交叉熵损失
    """
    epsilon = 1e-15  # 防止 log(0) 的情况
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=1))
    return loss
```






