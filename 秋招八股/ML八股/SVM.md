##### 1.优缺点
- 优点
	- 可处理高维数据：SVM 的最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维数灾难”。
	- 泛化能力强：SVM 通过最大化分类边界的间隔，具有较好的泛化能力，能够在面对新样本时产生较低的错误率。SVM 对异常值不敏感。
	- 可灵活选择核函数：SVM 可以根据数据的特点选择不同的核函数，如线性核、多项式核、高斯核等，使其适用于不同类型的数据。
	- 少数支持向量决定了最终结果，生成的模型节省存储空间。
	- SVM是一个凸优化问题，所以求得的解一定是全局最优而不是局部最优。其理论基础比较完善，不像神经网络更像一个黑盒子。
- 缺点
	- 对于大规模数据集，SVM的训练时间可能会非常长
	- 参数调节复杂
	- 只适用于二分类问题：传统的 SVM 只适用于二分类问题，对于多分类问题需要进行额外的处理，如 One-vs-All 或 One-vs-One。
##### 2.为什么要将求解 SVM 的原始问题转换为其对偶问题
- 对偶问题往往更易求解。为了使问题变得易于处理，我们的方法是把目标函数和约束全部融入拉格朗日函数，再通过这个函数来寻找最优点。
- 可以自然引入核函数，进而推广到非线性分类问题
##### 3.如何处理过拟合
- 引入松弛变量
- 加入正则化项
	- SVM是一个自带L2正则项的分类器，其防止过拟合的主要技巧就在于调整软间隔松弛变量的惩罚因子C。C越大表明越不能容忍错分，当无穷大时则退化为硬间隔分类器。
- 合理使用核函数
##### 4.什么是支持向量
- 支持向量是指在支持向量机（SVM）训练过程中，位于最大间隔边界上的训练样本点。它们对于确定决策边界起着重要作用。
- 在 SVM 中，支持向量的存在决定了决策边界的位置和形状。这些样本点是最接近不同类别之间的边界的样本，它们离决策边界最近，起到了支持和决策的作用。支持向量决定了最大间隔的边界的位置和宽度。
##### 5.如何处理高维数据
- 尽管SVM很擅长处理高维数据，但是当维度过于高时，仍然有问题
- 问题
	- 维度灾难：高维数据中特征的数量增加，会导致样本在特征空间中稀疏分布，这被称为维度灾难。在高维空间中，数据点之间的距离变得更远，导致SVM模型的性能下降。
	- 计算复杂度：随着数据维度的增加，计算和存储需求也会增加。特别是当使用核技巧处理非线性问题时，计算核矩阵（即，所有数据点对的核函数值）会变得非常昂贵。
- 可以采取以下策略：
	- 特征选择/降维
	- 核函数选择：选择合适的核函数，以避免过度复杂化模型。
	- 增加训练样本：如果可能的话，增加更多的训练样本可以帮助模型更好地学习高维空间中的数据分布，减少过拟合的风险
##### 6.模型推导
[19. 机器学习——朴素贝叶斯 - 牛客网 (nowcoder.com)](https://www.nowcoder.com/issue/tutorial?zhuanlanId=qMKkxM&uuid=739a130d75254de3bee6ae6cf9cca07f)
##### 7.为什么SVM对缺失数据敏感？
- SVM的分类效果和支持向量点有关，缺失值可能影响支持向量点的分布。如果缺失值不是支持向量，那么其实对模型影响不大；而如果缺失值刚好就是支持向量，那么会极大地影响模型效果；
##### 8.样本不均衡会影响效果吗
- 会，因为样本少的类别分布得不够广，所以分类超平面会靠近样本少的类别。
- 解决方法
	- 对多数类和和少数类采用不同的惩罚因子
	- 对训练集的数据进行预处理即对样本以某种策略进行采样，增加数量少的样本（上采样）或者减少数量多的样本（下采样）
##### 9.LR 和 SVM 联系与区别
- 联系：
	- 逻辑回归和支持向量机都是用于解决分类问题的监督学习算法。
	- 线性分类器：在二分类情况下，逻辑回归和线性核的支持向量机都是线性分类器，通过学习一个线性决策边界来进行分类。
	- 可解释性：逻辑回归和支持向量机都提供了对分类结果的解释，可以计算特征的权重或支持向量的重要性。
	- LR 和 SVM 都是判别式模型。
- 区别：
	- 决策边界形状：逻辑回归通过使用 sigmoid 函数将线性模型的输出转化为概率，因此决策边界可以是任意形状的曲线。而支持向量机通过最大化间隔来选择一个最优的决策边界，通常是线性的，但可以通过核函数引入非线性特征映射，从而处理非线性问题。
	- 目标函数：逻辑回归使用对数似然损失函数，通过最大化似然来拟合模型。而支持向量机通过最小化结构风险来寻找最优决策边界，其中结构风险包括经验误差和正则化项。
	- 处理噪声和异常值：逻辑回归对噪声和异常值比较敏感，因为它在似然函数中使用了所有训练样本。而支持向量机通过支持向量来定义决策边界，这些支持向量对噪声和异常值不敏感。
	- 多类分类问题：逻辑回归可以直接应用于多类分类问题，使用一对多（One-vs-All）策略。而支持向量机原始形式是二分类器，需要进行扩展，如一对多或一对一（One-vs-One）策略。
	- SVM 只考虑分界面附近的少数点，而 LR 则考虑所有点。
##### 10.为什么SVM要用hinge loss
![[1719112766461.png]]
- **间隔最大化**：
	- SVM 的核心思想是找到一个能够最大化正负类间隔的决策边界（超平面）。Hinge Loss 帮助实现这一目标：当样本被正确分类且离决策边界有足够的距离即 yif(xi)≥1时，损失为零，这鼓励模型不仅正确分类，还要尽量远离决策边界，从而最大化间隔。
- **对分类错误的样本进行惩罚**：
	- 当 yif(xi)<1时，Hinge Loss 对样本进行惩罚。
- **凸优化问题**：
    - Hinge Loss 是一个凸函数，这使得 SVM 的优化问题成为一个凸优化问题。凸优化问题具有唯一的全局最优解，确保了训练过程的稳定性和收敛性。
